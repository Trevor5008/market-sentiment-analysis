{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Price–News Join Analysis\n",
        "\n",
        "Analyze the GDELT–OHLCV join table: **news on day t → prices on day t+1** (next trading day).  \n",
        "We evaluate and validate the data, then compute mean and median sentiment (and optional price metrics) per ticker per day.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Find project root\n",
        "current = Path.cwd()\n",
        "while not (current / \"data\").exists() and current != current.parent:\n",
        "    current = current.parent\n",
        "PROJECT_ROOT = current\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "INPUT_PATH = PROCESSED_DIR / \"gdelt_ohlcv_join.csv\"\n",
        "\n",
        "df = pd.read_csv(\n",
        "    INPUT_PATH,\n",
        "    parse_dates=[\"seendate\", \"article_date\", \"price_date\"],\n",
        ")\n",
        "# Required for analysis\n",
        "required = [\"sentiment_score\", \"ticker\", \"article_date\", \"price_date\"]\n",
        "missing = [c for c in required if c not in df.columns]\n",
        "assert not missing, f\"Missing columns: {missing}\"\n",
        "print(f\"Loaded {len(df):,} rows from {INPUT_PATH.name}\")\n",
        "print(f\"Columns: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1A. Alternative Join (Trading-Day Indexed)\n",
        "\n",
        "Alternative approach for comparison:\n",
        "- Use `prices_daily_accumulated.csv` as the left/base table (one row per trading day per ticker).\n",
        "- Left-join `gdelt_articles_with_sentiment.csv` on `ticker` and calendar date.\n",
        "- This keeps the trading-day index intact and attaches any same-day news rows.\n",
        "\n",
        "This is separate from the existing `gdelt_ohlcv_join.csv` logic (`news t -> prices t+1`) and is intended for validation/EDA comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trading-day indexed left join: prices (left) <- gdelt accumulated\n",
        "prices_path = PROCESSED_DIR / \"prices_daily_accumulated.csv\"\n",
        "gdelt_acc_path = PROCESSED_DIR / \"gdelt_articles_with_sentiment.csv\"\n",
        "\n",
        "prices_left = pd.read_csv(prices_path, parse_dates=[\"date\"])\n",
        "gdelt_acc = pd.read_csv(gdelt_acc_path, parse_dates=[\"seendate\"])\n",
        "\n",
        "# Normalize join keys to calendar day + ticker\n",
        "prices_left[\"trading_date\"] = prices_left[\"date\"].dt.tz_localize(None).dt.normalize()\n",
        "gdelt_acc[\"article_date\"] = pd.to_datetime(gdelt_acc[\"seendate\"], utc=True).dt.tz_localize(None).dt.normalize()\n",
        "\n",
        "prices_left[\"ticker\"] = prices_left[\"ticker\"].astype(str).str.upper().str.strip()\n",
        "gdelt_acc[\"ticker\"] = gdelt_acc[\"ticker\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "alt_join = prices_left.merge(\n",
        "    gdelt_acc,\n",
        "    how=\"left\",\n",
        "    left_on=[\"trading_date\", \"ticker\"],\n",
        "    right_on=[\"article_date\", \"ticker\"],\n",
        "    suffixes=(\"_price\", \"_news\"),\n",
        ")\n",
        "\n",
        "print(\"Alternative join built:\")\n",
        "print(f\"  prices rows (left base): {len(prices_left):,}\")\n",
        "print(f\"  joined rows: {len(alt_join):,}\")\n",
        "print(f\"  trading date range: {prices_left['trading_date'].min().date()} -> {prices_left['trading_date'].max().date()}\")\n",
        "\n",
        "# Coverage diagnostics: how many trading-day x ticker rows have at least one matched article\n",
        "coverage = (\n",
        "    alt_join.groupby([\"trading_date\", \"ticker\"], as_index=False)\n",
        "    .agg(has_news=(\"url\", lambda s: s.notna().any()), article_rows=(\"url\", \"count\"))\n",
        ")\n",
        "\n",
        "print(f\"  unique (trading_date, ticker) pairs: {len(coverage):,}\")\n",
        "print(f\"  pairs with >=1 matched article: {int(coverage['has_news'].sum()):,}\")\n",
        "print(f\"  pairs with 0 matched articles: {int((~coverage['has_news']).sum()):,}\")\n",
        "\n",
        "print(\"\\nSample rows from alternative join:\")\n",
        "display(\n",
        "    alt_join[[\"trading_date\", \"ticker\", \"close\", \"seendate\", \"title\", \"sentiment_score\"]]\n",
        "    .sort_values([\"trading_date\", \"ticker\", \"seendate\"], na_position=\"last\")\n",
        "    .head(20)\n",
        ")\n",
        "\n",
        "# Keep as named object for downstream analysis cells if needed\n",
        "alt_join_df = alt_join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged = alt_join_df.sort_values(['ticker', 'trading_date'])\n",
        "\n",
        "# Create next day's close based on each ticker's last close\n",
        "merged['next_close'] = merged.groupby('ticker')['close'].shift(-1)\n",
        "# Calculate next day's return\n",
        "merged['next_return'] = (merged['next_close'] - merged['close']) / merged['close']\n",
        "# Calculate absolute return\n",
        "merged['abs_return'] = merged['next_return'].abs()\n",
        "\n",
        "daily = merged.groupby(['ticker', 'trading_date']).agg(\n",
        "    mean_sentiment=('sentiment_score', 'mean'),\n",
        "    median_sentiment=('sentiment_score', 'median'),\n",
        "    sentiment_count=('sentiment_present', 'sum'),\n",
        "    article_count=('title', 'count'),\n",
        "    next_return=('next_return', 'first'),\n",
        "    abs_return=('abs_return', 'first')\n",
        ").reset_index()\n",
        "daily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Questions:\n",
        "## Does sentiment predict price direction?\n",
        "## sentiment predict volatility?\n",
        "## sentiment bucket analysis\n",
        "## news volume effect\n",
        "## disagreement signal (sentiment range vs abs return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data evaluation and validation\n",
        "\n",
        "**First objective: validate the matching between prices and news** (join integrity, then schema and coverage)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema and shape\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nDtypes:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nKey columns present:\")\n",
        "key_cols = [\"article_date\", \"price_date\", \"ticker\", \"sentiment_score\", \"next_close\", \"next_volume\"]\n",
        "for c in key_cols:\n",
        "    print(f\"  {c}: {c in df.columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Validate price–news matching\n",
        "\n",
        "Check that **price_date** is the next trading day after **article_date** and that attached prices match the source OHLCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) price_date must be strictly after article_date (next trading day)\n",
        "df[\"_gap_days\"] = (df[\"price_date\"] - df[\"article_date\"]).dt.days\n",
        "bad_order = (df[\"_gap_days\"] <= 0).sum()\n",
        "print(\"1) Article date → price date (next trading day)\")\n",
        "# Display number of misaligned rows (s/b 0)\n",
        "print(f\"   Rows where price_date ≤ article_date: {bad_order} (expect 0)\")\n",
        "# Unit test for correct matching\n",
        "assert bad_order == 0, \"Every row must have price_date > article_date\"\n",
        "print(\"   ✓ All rows have price_date after article_date\")\n",
        "print(\"\\n   Calendar-day gap (article_date to price_date):\")\n",
        "print(df[\"_gap_days\"].value_counts().sort_index().to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Within join: each (price_date, ticker) should have exactly one set of next_* values (no conflicts)\n",
        "price_cols = [c for c in df.columns if c.startswith(\"next_\")]\n",
        "check = df.groupby([\"price_date\", \"ticker\"])[price_cols].nunique()\n",
        "max_per_col = check.max()\n",
        "conflicts = (check > 1).any(axis=1).sum()\n",
        "print(\"2) One price per (price_date, ticker)\")\n",
        "print(f\"   Unique (price_date, ticker) pairs: {len(check):,}\")\n",
        "print(f\"   Pairs with conflicting next_* values: {conflicts} (expect 0)\")\n",
        "# Unit test for unique prices per (price_date, ticker)\n",
        "assert conflicts == 0, \"⚠ Some (price_date, ticker) have multiple different prices — investigate\"\n",
        "print(\"   ✓ All rows for same (price_date, ticker) have identical next_* values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Cross-check: join next_* values vs source OHLCV (prices_daily_accumulated)\n",
        "ohlcv_path = PROCESSED_DIR / \"prices_daily_accumulated.csv\"\n",
        "if ohlcv_path.exists():\n",
        "    ohlcv = pd.read_csv(ohlcv_path, parse_dates=[\"date\"])\n",
        "    # One row per (date, ticker) in join; take first next_* per (price_date, ticker)\n",
        "    join_prices = df.groupby([\"price_date\", \"ticker\"])[\"next_close\"].first().reset_index()\n",
        "    join_prices = join_prices.rename(columns={\"price_date\": \"date\", \"next_close\": \"join_close\"})\n",
        "    merged = join_prices.merge(ohlcv[[\"date\", \"ticker\", \"close\"]], on=[\"date\", \"ticker\"], how=\"left\")\n",
        "    merged[\"match\"] = merged[\"join_close\"].round(6) == merged[\"close\"].round(6)\n",
        "    mismatches = (~merged[\"match\"]).sum()\n",
        "    missing = merged[\"close\"].isna().sum()\n",
        "    print(\"3) Join vs source OHLCV (next_close vs close)\")\n",
        "    print(f\"   (price_date, ticker) pairs checked: {len(merged):,}\")\n",
        "    print(f\"   Mismatches (join next_close ≠ OHLCV close): {mismatches}\")\n",
        "    print(f\"   Missing in OHLCV: {missing}\")\n",
        "    # Unit test for matching prices\n",
        "    assert mismatches == 0 and missing == 0, \"   ⚠ Review mismatches or missing dates\"\n",
        "    print(\"   ✓ All join prices match source OHLCV\")\n",
        "else:\n",
        "    print(\"3) Skip cross-check (prices_daily_accumulated.csv not found)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Schema, date ranges, and coverage\n",
        "\n",
        "Schema, date ranges, missing values, and ticker coverage (for downstream aggregation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Date ranges and join alignment\n",
        "art_min, art_max = df[\"article_date\"].min(), df[\"article_date\"].max()\n",
        "price_min, price_max = df[\"price_date\"].min(), df[\"price_date\"].max()\n",
        "print(\"Article date range:\", art_min.date(), \"to\", art_max.date())\n",
        "print(\"Price date range: \", price_min.date(), \"to\", price_max.date())\n",
        "print(\"\\nExpected: price_date = next trading day after article_date (weekends/holidays skipped).\")\n",
        "# Spot-check: article_date and price_date should differ by 1–3 calendar days (Fri→Mon = 3)\n",
        "df[\"days_to_next\"] = (df[\"price_date\"] - df[\"article_date\"]).dt.days\n",
        "print(\"\\nCalendar days from article_date to price_date (sample):\")\n",
        "print(df[\"days_to_next\"].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values in columns used for aggregation\n",
        "agg_cols = [\"sentiment_score\", \"ticker\", \"article_date\", \"price_date\"]\n",
        "if \"next_close\" in df.columns:\n",
        "    agg_cols.append(\"next_close\")\n",
        "missing = df[agg_cols].isna().sum()\n",
        "print(\"Missing values (columns used for mean/median per ticker per day):\")\n",
        "print(missing[missing > 0] if missing.any() else \"  None\")\n",
        "print(\"\\nRows with any missing in these columns:\", df[agg_cols].isna().any(axis=1).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ticker coverage: articles and (article_date, ticker) pairs\n",
        "print(\"Articles per ticker:\")\n",
        "print(df[\"ticker\"].value_counts().sort_index())\n",
        "print(\"\\nUnique (article_date, ticker) pairs per ticker = distinct calendar days with ≥1 article:\")\n",
        "# Per ticker, count distinct article_date (same as count of (article_date, ticker) per ticker)\n",
        "unique_days_per_ticker = df.groupby(\"ticker\")[\"article_date\"].nunique()\n",
        "print(unique_days_per_ticker.to_string())\n",
        "print(\"\\n(Multiple articles per day per ticker is expected; we aggregate to mean/median per (date, ticker) later.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute mean/median sentiment per ticker per day\n",
        "sentiment_per_ticker_per_day = df.groupby([\"ticker\", \"price_date\"])[\"sentiment_score\"].agg([\"mean\", \"median\"])\n",
        "print(\"\\nMean and median sentiment per ticker per day:\")\n",
        "print(sentiment_per_ticker_per_day.head())\n",
        "\n",
        "# Sanity check aggregated outputs across tickers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sentiment summary statistics per ticker per day\n",
        "\n",
        "Aggregate to mean and median sentiment per (ticker, day)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate: one row per (article_date, ticker)\n",
        "daily = df.groupby([\"article_date\", \"ticker\"]).agg(\n",
        "    sentiment_mean=(\"sentiment_score\", \"mean\"),\n",
        "    sentiment_median=(\"sentiment_score\", \"median\"),\n",
        "    article_count=(\"sentiment_score\", \"count\"),\n",
        ").reset_index()\n",
        "if \"next_close\" in df.columns:\n",
        "    daily[\"next_close\"] = df.groupby([\"article_date\", \"ticker\"])[\"next_close\"].first().values\n",
        "if \"next_volume\" in df.columns:\n",
        "    daily[\"next_volume\"] = df.groupby([\"article_date\", \"ticker\"])[\"next_volume\"].first().values\n",
        "\n",
        "print(\"Daily summary (first rows):\")\n",
        "daily.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Monday-only analysis: sentiment per ticker per week\n",
        "\n",
        "Filter to **Mondays only** (article_date), then group by week (7-day increments starting from **2026-01-12** as week 1). Compute summary statistics per ticker per week (open-ended; weeks 1+)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to Mondays only (dayofweek: Monday=0)\n",
        "mondays = daily[daily[\"article_date\"].dt.dayofweek == 0].copy()\n",
        "print(f\"Total rows: {len(daily):,}\")\n",
        "print(f\"Mondays only: {len(mondays):,} ({100*len(mondays)/len(daily):.1f}%)\")\n",
        "print(f\"\\nMonday dates in data:\")\n",
        "monday_dates = sorted(mondays[\"article_date\"].dt.date.unique())\n",
        "print(monday_dates)\n",
        "print(f\"\\nExpected Mondays (if all weeks present):\")\n",
        "week_start = pd.Timestamp(\"2026-01-12\")\n",
        "for w in range(1, 6):  # weeks 1-5 based on data up to 2/9\n",
        "    expected_monday = week_start + pd.Timedelta(days=7*(w-1))\n",
        "    print(f\"  Week {w}: {expected_monday.date()}\")\n",
        "    if expected_monday.date() not in monday_dates:\n",
        "        print(f\"    ⚠ MISSING from data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign week number (increments of 7 days starting from 2026-01-12)\n",
        "# Week 1 = 2026-01-12 + 0-6 days, Week 2 = 2026-01-12 + 7-13 days, etc.\n",
        "week_start_date = pd.Timestamp(\"2026-01-12\")\n",
        "mondays[\"days_since_week1_start\"] = (mondays[\"article_date\"] - week_start_date).dt.days\n",
        "mondays[\"week\"] = (mondays[\"days_since_week1_start\"] // 7) + 1\n",
        "# Explain missing weeks (MLK holiday on 1/19)\n",
        "print(\"Note: MLK holiday on 1/19 causes week 2 to be missing\")\n",
        "# Filter to weeks >= 1 (exclude dates before 1/12, but no upper limit - open-ended)\n",
        "mondays_filtered = mondays[mondays[\"week\"] >= 1].copy()\n",
        "print(f\"Week start date: {week_start_date.date()}\")\n",
        "print(f\"Week range in data: {mondays_filtered['week'].min()} to {mondays_filtered['week'].max()} (open-ended)\")\n",
        "print(f\"\\nMondays per week:\")\n",
        "week_counts = mondays_filtered.groupby(\"week\")[\"article_date\"].nunique()\n",
        "print(week_counts)\n",
        "print(f\"\\nMissing weeks (expected but not present):\")\n",
        "all_weeks = set(range(mondays_filtered['week'].min(), mondays_filtered['week'].max() + 1))\n",
        "present_weeks = set(week_counts.index)\n",
        "missing_weeks = sorted(all_weeks - present_weeks)\n",
        "if missing_weeks:\n",
        "    for w in missing_weeks:\n",
        "        expected_date = week_start_date + pd.Timedelta(days=7*(w-1))\n",
        "        print(f\"  Week {w}: {expected_date.date()} (no articles on this Monday)\")\n",
        "        # Check if this date exists in the raw daily data (not just Mondays)\n",
        "        if expected_date.date() in daily[\"article_date\"].dt.date.values:\n",
        "            print(f\"    → Date exists in daily data but is not a Monday (dayofweek check)\")\n",
        "        else:\n",
        "            print(f\"    → Date not in daily data at all (no articles on this date)\")\n",
        "else:\n",
        "    print(\"  None\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics per ticker per week (Mondays only)\n",
        "# Use mondays_filtered to exclude week 0 (dates before 1/12)\n",
        "weekly_stats = mondays_filtered.groupby([\"ticker\", \"week\"]).agg(\n",
        "    sentiment_mean=(\"sentiment_mean\", \"mean\"),\n",
        "    sentiment_median=(\"sentiment_median\", \"median\"),\n",
        "    # sentiment_std excluded: std of already-aggregated daily means is problematic (NaN when only one Monday)\n",
        "    monday_count=(\"article_date\", \"nunique\"),  # number of Mondays in this week with data\n",
        "    total_articles=(\"article_count\", \"sum\"),\n",
        ").reset_index()\n",
        "\n",
        "print(\"Summary statistics per ticker per week (Mondays only, weeks 1+):\")\n",
        "print(\"=\" * 80)\n",
        "weekly_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pivot table for mean sentiment per ticker (per week) - weeks 1+ (open-ended)\n",
        "weekly_pivot_mean = weekly_stats.pivot(index=\"ticker\", columns=\"week\", values=\"sentiment_mean\")\n",
        "print(\"Mean sentiment per ticker per week (Mondays only, weeks 1+) - pivot view:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Note: Week 0 (dates before 1/12) is excluded. If a ticker has NaN for a week,\")\n",
        "print(\"      that ticker had no articles on the Monday(s) in that week.\")\n",
        "print(\"      Weeks are open-ended; new weeks will appear as data extends past 2/9.\")\n",
        "weekly_pivot_mean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pivot table for median sentiment per ticker (per week)\n",
        "weekly_pivot_median = weekly_stats.pivot(index=\"ticker\", columns=\"week\", values=\"sentiment_median\")\n",
        "print(\"Median sentiment per ticker per week (Mondays only) - pivot view:\")\n",
        "print(\"=\" * 70)\n",
        "# Display table\n",
        "weekly_pivot_median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (advds)",
      "language": "python",
      "name": "advds"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
