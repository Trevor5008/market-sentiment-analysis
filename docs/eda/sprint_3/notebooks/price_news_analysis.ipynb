{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Price–News Join Analysis\n",
        "\n",
        "Analyze the GDELT–OHLCV join table: **news on day t → prices on day t+1** (next trading day).  \n",
        "We evaluate and validate the data, then compute mean and median sentiment (and optional price metrics) per ticker per day.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Find project root\n",
        "current = Path.cwd()\n",
        "while not (current / \"data\").exists() and current != current.parent:\n",
        "    current = current.parent\n",
        "PROJECT_ROOT = current\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "INPUT_PATH = PROCESSED_DIR / \"gdelt_ohlcv_join.csv\"\n",
        "\n",
        "df = pd.read_csv(\n",
        "    INPUT_PATH,\n",
        "    parse_dates=[\"seendate\", \"article_date\", \"price_date\"],\n",
        ")\n",
        "# Required for analysis\n",
        "required = [\"sentiment_score\", \"ticker\", \"article_date\", \"price_date\"]\n",
        "missing = [c for c in required if c not in df.columns]\n",
        "assert not missing, f\"Missing columns: {missing}\"\n",
        "print(f\"Loaded {len(df):,} rows from {INPUT_PATH.name}\")\n",
        "print(f\"Columns: {list(df.columns)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 2,591 rows from gdelt_ohlcv_join.csv\n",
            "Columns: ['seendate', 'url', 'title', 'language', 'domain', 'socialimage', 'company', 'ticker', 'sentiment_score', 'sentiment_hits', 'sentiment_present', 'article_date', 'price_date', 'next_open', 'next_high', 'next_low', 'next_close', 'next_adj_close', 'next_volume']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data evaluation and validation\n",
        "\n",
        "**First objective: validate the matching between prices and news** (join integrity, then schema and coverage)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Schema and shape\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nDtypes:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nKey columns present:\")\n",
        "key_cols = [\"article_date\", \"price_date\", \"ticker\", \"sentiment_score\", \"next_close\", \"next_volume\"]\n",
        "for c in key_cols:\n",
        "    print(f\"  {c}: {c in df.columns}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (2591, 19)\n",
            "\n",
            "Dtypes:\n",
            "seendate             datetime64[us, UTC]\n",
            "url                                  str\n",
            "title                                str\n",
            "language                             str\n",
            "domain                               str\n",
            "socialimage                          str\n",
            "company                              str\n",
            "ticker                               str\n",
            "sentiment_score                  float64\n",
            "sentiment_hits                   float64\n",
            "sentiment_present                   bool\n",
            "article_date              datetime64[us]\n",
            "price_date                datetime64[us]\n",
            "next_open                        float64\n",
            "next_high                        float64\n",
            "next_low                         float64\n",
            "next_close                       float64\n",
            "next_adj_close                   float64\n",
            "next_volume                        int64\n",
            "dtype: object\n",
            "\n",
            "Key columns present:\n",
            "  article_date: True\n",
            "  price_date: True\n",
            "  ticker: True\n",
            "  sentiment_score: True\n",
            "  next_close: True\n",
            "  next_volume: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Validate price–news matching\n",
        "\n",
        "Check that **price_date** is the next trading day after **article_date** and that attached prices match the source OHLCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1) price_date must be strictly after article_date (next trading day)\n",
        "df[\"_gap_days\"] = (df[\"price_date\"] - df[\"article_date\"]).dt.days\n",
        "bad_order = (df[\"_gap_days\"] <= 0).sum()\n",
        "print(\"1) Article date → price date (next trading day)\")\n",
        "print(f\"   Rows where price_date ≤ article_date: {bad_order} (expect 0)\")\n",
        "assert bad_order == 0, \"Every row must have price_date > article_date\"\n",
        "print(\"   ✓ All rows have price_date after article_date\")\n",
        "print(\"\\n   Calendar-day gap (article_date to price_date):\")\n",
        "print(df[\"_gap_days\"].value_counts().sort_index().to_string())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1) Article date → price date (next trading day)\n",
            "   Rows where price_date ≤ article_date: 0 (expect 0)\n",
            "   ✓ All rows have price_date after article_date\n",
            "\n",
            "   Calendar-day gap (article_date to price_date):\n",
            "_gap_days\n",
            "1    2462\n",
            "2      74\n",
            "3      55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2) Within join: each (price_date, ticker) should have exactly one set of next_* values (no conflicts)\n",
        "price_cols = [c for c in df.columns if c.startswith(\"next_\")]\n",
        "check = df.groupby([\"price_date\", \"ticker\"])[price_cols].nunique()\n",
        "max_per_col = check.max()\n",
        "conflicts = (check > 1).any(axis=1).sum()\n",
        "print(\"2) One price per (price_date, ticker)\")\n",
        "print(f\"   Unique (price_date, ticker) pairs: {len(check):,}\")\n",
        "print(f\"   Pairs with conflicting next_* values: {conflicts} (expect 0)\")\n",
        "if conflicts == 0:\n",
        "    print(\"   ✓ All rows for same (price_date, ticker) have identical next_* values\")\n",
        "else:\n",
        "    print(\"   ⚠ Some (price_date, ticker) have multiple different prices — investigate\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2) One price per (price_date, ticker)\n",
            "   Unique (price_date, ticker) pairs: 88\n",
            "   Pairs with conflicting next_* values: 0 (expect 0)\n",
            "   ✓ All rows for same (price_date, ticker) have identical next_* values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3) Cross-check: join next_* values vs source OHLCV (prices_daily_accumulated)\n",
        "ohlcv_path = PROCESSED_DIR / \"prices_daily_accumulated.csv\"\n",
        "if ohlcv_path.exists():\n",
        "    ohlcv = pd.read_csv(ohlcv_path, parse_dates=[\"date\"])\n",
        "    # One row per (date, ticker) in join; take first next_* per (price_date, ticker)\n",
        "    join_prices = df.groupby([\"price_date\", \"ticker\"])[\"next_close\"].first().reset_index()\n",
        "    join_prices = join_prices.rename(columns={\"price_date\": \"date\", \"next_close\": \"join_close\"})\n",
        "    merged = join_prices.merge(ohlcv[[\"date\", \"ticker\", \"close\"]], on=[\"date\", \"ticker\"], how=\"left\")\n",
        "    merged[\"match\"] = merged[\"join_close\"].round(6) == merged[\"close\"].round(6)\n",
        "    mismatches = (~merged[\"match\"]).sum()\n",
        "    missing = merged[\"close\"].isna().sum()\n",
        "    print(\"3) Join vs source OHLCV (next_close vs close)\")\n",
        "    print(f\"   (price_date, ticker) pairs checked: {len(merged):,}\")\n",
        "    print(f\"   Mismatches (join next_close ≠ OHLCV close): {mismatches}\")\n",
        "    print(f\"   Missing in OHLCV: {missing}\")\n",
        "    if mismatches == 0 and missing == 0:\n",
        "        print(\"   ✓ All join prices match source OHLCV\")\n",
        "    else:\n",
        "        print(\"   ⚠ Review mismatches or missing dates\")\n",
        "else:\n",
        "    print(\"3) Skip cross-check (prices_daily_accumulated.csv not found)\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3) Join vs source OHLCV (next_close vs close)\n",
            "   (price_date, ticker) pairs checked: 88\n",
            "   Mismatches (join next_close ≠ OHLCV close): 0\n",
            "   Missing in OHLCV: 0\n",
            "   ✓ All join prices match source OHLCV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Schema, date ranges, and coverage\n",
        "\n",
        "Schema, date ranges, missing values, and ticker coverage (for downstream aggregation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Date ranges and join alignment\n",
        "art_min, art_max = df[\"article_date\"].min(), df[\"article_date\"].max()\n",
        "price_min, price_max = df[\"price_date\"].min(), df[\"price_date\"].max()\n",
        "print(\"Article date range:\", art_min.date(), \"to\", art_max.date())\n",
        "print(\"Price date range: \", price_min.date(), \"to\", price_max.date())\n",
        "print(\"\\nExpected: price_date = next trading day after article_date (weekends/holidays skipped).\")\n",
        "# Spot-check: article_date and price_date should differ by 1–3 calendar days (Fri→Mon = 3)\n",
        "df[\"days_to_next\"] = (df[\"price_date\"] - df[\"article_date\"]).dt.days\n",
        "print(\"\\nCalendar days from article_date to price_date (sample):\")\n",
        "print(df[\"days_to_next\"].value_counts().head(10))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article date range: 2026-01-05 to 2026-02-08\n",
            "Price date range:  2026-01-06 to 2026-02-09\n",
            "\n",
            "Expected: price_date = next trading day after article_date (weekends/holidays skipped).\n",
            "\n",
            "Calendar days from article_date to price_date (sample):\n",
            "days_to_next\n",
            "1    2462\n",
            "2      74\n",
            "3      55\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Missing values in columns used for aggregation\n",
        "agg_cols = [\"sentiment_score\", \"ticker\", \"article_date\", \"price_date\"]\n",
        "if \"next_close\" in df.columns:\n",
        "    agg_cols.append(\"next_close\")\n",
        "missing = df[agg_cols].isna().sum()\n",
        "print(\"Missing values (columns used for mean/median per ticker per day):\")\n",
        "print(missing[missing > 0] if missing.any() else \"  None\")\n",
        "print(\"\\nRows with any missing in these columns:\", df[agg_cols].isna().any(axis=1).sum())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing values (columns used for mean/median per ticker per day):\n",
            "  None\n",
            "\n",
            "Rows with any missing in these columns: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ticker coverage: articles and (article_date, ticker) pairs\n",
        "print(\"Articles per ticker:\")\n",
        "print(df[\"ticker\"].value_counts().sort_index())\n",
        "print(\"\\nUnique (article_date, ticker) pairs per ticker = distinct calendar days with ≥1 article:\")\n",
        "# Per ticker, count distinct article_date (same as count of (article_date, ticker) per ticker)\n",
        "unique_days_per_ticker = df.groupby(\"ticker\")[\"article_date\"].nunique()\n",
        "print(unique_days_per_ticker.to_string())\n",
        "print(\"\\n(Multiple articles per day per ticker is expected; we aggregate to mean/median per (date, ticker) later.)\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Articles per ticker:\n",
            "ticker\n",
            "AAPL     232\n",
            "AMZN     269\n",
            "GOOGL    528\n",
            "META     478\n",
            "MSFT     290\n",
            "NVDA     564\n",
            "TSLA     230\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Unique (article_date, ticker) pairs per ticker (calendar days with at least one article):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "Index(['ticker'], dtype='str')",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[32m/tmp/ipykernel_7619/1998101310.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ticker coverage: articles and (article_date, ticker) pairs\u001b[39;00m\n\u001b[32m      2\u001b[39m print(\u001b[33m\"Articles per ticker:\"\u001b[39m)\n\u001b[32m      3\u001b[39m print(df[\u001b[33m\"ticker\"\u001b[39m].value_counts().sort_index())\n\u001b[32m      4\u001b[39m print(\u001b[33m\"\\nUnique (article_date, ticker) pairs per ticker (calendar days with at least one article):\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m print(df.groupby(\u001b[33m\"ticker\"\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m g: g.drop_duplicates([\u001b[33m\"article_date\"\u001b[39m, \u001b[33m\"ticker\"\u001b[39m]).shape[\u001b[32m0\u001b[39m]))\n\u001b[32m      6\u001b[39m print(\u001b[33m\"\\nValidation: no duplicate (article_date, ticker) in raw join — multiple articles per day per ticker is expected; we will aggregate to mean/median per (date, ticker) next.\"\u001b[39m)\n",
            "\u001b[32m~/miniconda3/envs/advds/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1646\u001b[39m                 )\n\u001b[32m   1647\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1648\u001b[39m             f = func\n\u001b[32m   1649\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._python_apply_general(f, self._obj_with_exclusions)\n",
            "\u001b[32m~/miniconda3/envs/advds/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1683\u001b[39m         -------\n\u001b[32m   1684\u001b[39m         Series \u001b[38;5;28;01mor\u001b[39;00m DataFrame\n\u001b[32m   1685\u001b[39m             data after applying f\n\u001b[32m   1686\u001b[39m         \"\"\"\n\u001b[32m-> \u001b[39m\u001b[32m1687\u001b[39m         values, mutated = self._grouper.apply_groupwise(f, data)\n\u001b[32m   1688\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1689\u001b[39m             not_indexed_same = mutated\n\u001b[32m   1690\u001b[39m \n",
            "\u001b[32m~/miniconda3/envs/advds/lib/python3.11/site-packages/pandas/core/groupby/ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, f, data)\u001b[39m\n\u001b[32m   1025\u001b[39m             object.__setattr__(group, \u001b[33m\"name\"\u001b[39m, key)\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m             \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m   1028\u001b[39m             group_axes = group.axes\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m             res = f(group)\n\u001b[32m   1030\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m mutated \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m _is_indexed_like(res, group_axes):\n\u001b[32m   1031\u001b[39m                 mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1032\u001b[39m             result_values.append(res)\n",
            "\u001b[32m/tmp/ipykernel_7619/1998101310.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(g)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m print(df.groupby(\u001b[33m\"ticker\"\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m g: g.drop_duplicates([\u001b[33m\"article_date\"\u001b[39m, \u001b[33m\"ticker\"\u001b[39m]).shape[\u001b[32m0\u001b[39m]))\n",
            "\u001b[32m~/miniconda3/envs/advds/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, subset, keep, inplace, ignore_index)\u001b[39m\n\u001b[32m   7937\u001b[39m \n\u001b[32m   7938\u001b[39m         inplace = validate_bool_kwarg(inplace, \u001b[33m\"inplace\"\u001b[39m)\n\u001b[32m   7939\u001b[39m         ignore_index = validate_bool_kwarg(ignore_index, \u001b[33m\"ignore_index\"\u001b[39m)\n\u001b[32m   7940\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7941\u001b[39m         result = self[-self.duplicated(subset, keep=keep)]\n\u001b[32m   7942\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[32m   7943\u001b[39m             result.index = default_index(len(result))\n\u001b[32m   7944\u001b[39m \n",
            "\u001b[32m~/miniconda3/envs/advds/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, subset, keep)\u001b[39m\n\u001b[32m   8067\u001b[39m         \u001b[38;5;66;03m# Otherwise, raise a KeyError, same as if you try to __getitem__ with a\u001b[39;00m\n\u001b[32m   8068\u001b[39m         \u001b[38;5;66;03m# key that doesn't exist.\u001b[39;00m\n\u001b[32m   8069\u001b[39m         diff = set(subset) - set(self.columns)\n\u001b[32m   8070\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[32m-> \u001b[39m\u001b[32m8071\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(Index(diff))\n\u001b[32m   8072\u001b[39m \n\u001b[32m   8073\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(subset) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m self.columns.is_unique:\n\u001b[32m   8074\u001b[39m             \u001b[38;5;66;03m# GH#45236 This is faster than get_group_index below\u001b[39;00m\n",
            "\u001b[31mKeyError\u001b[39m: Index(['ticker'], dtype='str')"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (advds)",
      "language": "python",
      "name": "advds"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}