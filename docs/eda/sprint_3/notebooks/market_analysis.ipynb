{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7924f97",
   "metadata": {},
   "source": [
    "## Market Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8ad37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../../../data/processed/gdelt_ohlcv_join.csv...\n",
      "File not found at ../../../data/processed/gdelt_ohlcv_join.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Replace with your actual file path\n",
    "FILE_PATH = \"../../../../data/processed/gdelt_ohlcv_join.csv\" \n",
    "\n",
    "def analyze_market_regimes(file_path):\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        # Fallback for notebook relative paths or missing files\n",
    "        print(f\"File not found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    # map specific column names to the standard names the script needs.\n",
    "    rename_map = {\n",
    "        'price_date': 'date',\n",
    "        # 'article_date': 'date',    # Fallback if price_date is missing\n",
    "        'next_close': 'close',\n",
    "        'Close': 'close',          # Fallback\n",
    "        'next_volume': 'volume',\n",
    "        'Volume': 'volume',        # Fallback\n",
    "        'next_open': 'open',\n",
    "        'Open': 'open'             # Fallback\n",
    "    }\n",
    "    \n",
    "    # Apply the renaming\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    # VALIDATION: Check if we have the Big 3 columns\n",
    "    required_cols = ['date', 'close', 'volume']\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"   CRITICAL ERROR: Could not find columns: {missing}\")\n",
    "        print(f\"   Your available columns are: {df.columns.tolist()}\")\n",
    "        return\n",
    "\n",
    "    # Standardize Dates\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['ticker', 'date'])\n",
    "    \n",
    "    print(f\" Data Loaded Successfully: {len(df)} rows.\")\n",
    "\n",
    "    # Close-Close % Return\n",
    "    df['daily_return'] = df.groupby('ticker')['close'].pct_change()\n",
    "    \n",
    "    # Absolute Return (Magnitude of move, directionless)\n",
    "    df['abs_return'] = df['daily_return'].abs()\n",
    "    \n",
    "    # We use a 20 day Rolling Standard Deviation of returns (Annualized)\n",
    "    df['volatility_proxy'] = df.groupby('ticker')['daily_return'].transform(\n",
    "        lambda x: x.rolling(window=20).std()\n",
    "    )\n",
    "\n",
    "    # Drop NaN values generated by shifting/rolling\n",
    "    df = df.dropna(subset=['daily_return', 'volatility_proxy'])\n",
    "\n",
    "    # Sentiment Buckets\n",
    "    # Logic: \"High\" is top 33%, \"Low\" is bottom 33% relative to ticker's history.\n",
    "    \n",
    "    def bucket_sentiment(group):\n",
    "        try:\n",
    "            # qcut to divide into equal sized buckets\n",
    "            return pd.qcut(group['sentiment_score'], 3, labels=[\"Low\", \"Neutral\", \"High\"])\n",
    "        except ValueError:\n",
    "            # Fallback if all sentiment scores are identical\n",
    "            return pd.Series([\"Neutral\"] * len(group), index=group.index)\n",
    "\n",
    "    if 'sentiment_score' in df.columns:\n",
    "        df['sentiment_bucket'] = df.groupby('ticker').apply(bucket_sentiment).reset_index(level=0, drop=True)\n",
    "    else:\n",
    "        print(\" Warning: 'sentiment_score' column missing. Skipping sentiment buckets.\")\n",
    "\n",
    "    # Volume Buckets\n",
    "    # RVOL = Today's Vol / 20-Day Average Vol\n",
    "    df['avg_volume_20d'] = df.groupby('ticker')['volume'].transform(lambda x: x.rolling(20).mean())\n",
    "    df['rvol'] = df['volume'] / df['avg_volume_20d']\n",
    "    \n",
    "    # Bucket: Low (< 0.8x), Normal (0.8x - 1.2x), High (> 1.2x)\n",
    "    bins = [-np.inf, 0.8, 1.2, np.inf]\n",
    "    labels = ['Low Vol', 'Normal Vol', 'High Vol']\n",
    "    df['volume_bucket'] = pd.cut(df['rvol'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "    print(\"\\n==================================================\")\n",
    "    print(\" MARKET REGIME REPORT\")\n",
    "    print(\"==================================================\")\n",
    "    \n",
    "    # 1. Volatility Inspection\n",
    "    avg_vol = df.groupby('ticker')['volatility_proxy'].mean() * np.sqrt(252) # Annualized\n",
    "    print(f\"\\n1. ðŸ“‰ Average Annualized Volatility:\")\n",
    "    print(avg_vol.to_markdown())\n",
    "\n",
    "    # 2. Return Distribution by Sentiment\n",
    "    if 'sentiment_bucket' in df.columns:\n",
    "        print(f\"\\n2.  Return Distribution by Sentiment Bucket:\")\n",
    "        sent_stats = df.groupby('sentiment_bucket')['daily_return'].agg(['mean', 'std', 'count'])\n",
    "        print(sent_stats.to_markdown())\n",
    "        \n",
    "        try:\n",
    "            if sent_stats.loc['High', 'mean'] > sent_stats.loc['Low', 'mean']:\n",
    "                print(\"\\n    Insight: 'High Sentiment' days have higher average returns.\")\n",
    "            else:\n",
    "                print(\"\\n    Insight: Sentiment does not clearly differentiate returns.\")\n",
    "        except KeyError:\n",
    "            pass \n",
    "\n",
    "    # 3. Return Distribution by Volume\n",
    "    print(f\"\\n3.  Return Distribution by Volume Bucket:\")\n",
    "    vol_stats = df.groupby('volume_bucket')['abs_return'].agg(['mean', 'count'])\n",
    "    vol_stats.columns = ['Avg Absolute Move', 'Count']\n",
    "    print(vol_stats.to_markdown())\n",
    "    \n",
    "    print(\"\\n   (Note: High Volume usually corresponds to larger Absolute Moves/Volatility)\")\n",
    "\n",
    "    # VISUALIZATION \n",
    "    plot_distributions(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_distributions(df):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot 1: Sentiment vs Returns\n",
    "    if 'sentiment_bucket' in df.columns:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.boxplot(x='sentiment_bucket', y='daily_return', hue='sentiment_bucket', data=df, palette=\"coolwarm\", showfliers=False, legend=False)\n",
    "        plt.title(\"Does Sentiment Impact Daily Returns?\")\n",
    "        plt.axhline(0, color='black', linestyle='--')\n",
    "        plt.ylabel(\"Daily Return\")\n",
    "\n",
    "    # Plot 2: Volume vs Volatility (Absolute Return)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x='volume_bucket', y='abs_return', hue='volume_bucket', data=df, palette=\"viridis\", legend=False)\n",
    "    plt.title(\"Does Volume Impact Price Movement Size?\")\n",
    "    plt.ylabel(\"Avg Absolute Return (Magnitude)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_regime = analyze_market_regimes(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107443f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
