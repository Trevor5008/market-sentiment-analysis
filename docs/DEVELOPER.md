# Developer Reference

Technical documentation for developers working on the Market Sentiment Analysis project: setup, pipeline, scripts, and common tasks.

## Table of Contents

- [Architecture Overview](#architecture-overview)
- [Module Documentation](#module-documentation)
- [Data Pipeline](#data-pipeline)
- [Development Workflow](#development-workflow)
- [Code Style & Standards](#code-style--standards)
- [Common Tasks](#common-tasks)
- [Troubleshooting](#troubleshooting)
- [API Reference](#api-reference)

---

## Architecture Overview

### Project Structure

```text
market-sentiment-analysis/
├── data/
│   ├── raw/                    # Canonical raw files (tracked); archive/, snapshots/ (gitignored)
│   │   ├── gdelt_articles.csv
│   │   ├── prices_daily.csv
│   │   ├── archive/            # Date-stamped backups (gitignored)
│   │   └── snapshots/          # Run manifests run_manifest_YYYY-MM-DD.json (gitignored)
│   └── processed/              # Cleaned + accumulated outputs
│       ├── gdelt_articles_clean.csv
│       ├── gdelt_articles_accumulated.csv
│       ├── prices_daily_clean.csv
│       ├── prices_daily_accumulated.csv
│       └── *_manifest.json    # Accumulation manifests (gitignored)
├── scripts/
│   ├── data_ingestion.py       # Fetch GDELT + OHLCV; archive previous; write run manifest
│   ├── validate_gdelt.py       # Validate GDELT; report to docs/validation/
│   ├── cleaning_gdelt.py      # Clean GDELT → gdelt_articles_clean.csv (URL + headline dedupe)
│   ├── ohlcv_validation.py    # Validate OHLCV; report to docs/validation/
│   ├── ohlcv_cleaning.py      # Clean OHLCV → prices_daily_clean.csv
│   ├── accumulate.py          # Merge new cleaned + existing accumulated; dedupe; sort by date
│   └── run_pipeline.sh        # Orchestrates validate → clean → accumulate; optional ingestion (RUN_INGEST=1)
├── notebooks/                  # Exploratory notebooks (root)
├── docs/
│   ├── architecture/           # Pipeline diagram and docs (pipeline.md, pipeline.svg)
│   ├── eda/                    # EDA notebooks and summaries by sprint
│   │   └── sprint_2/
│   │       └── notebooks/      # e.g. weekend_effect_analysis.ipynb
│   ├── findings/               # Written findings (e.g. weekend_effect_analysis.md)
│   ├── validation/             # Validation reports (gitignored; regenerated by scripts)
│   ├── data_snapshot_log.md    # Log of canonical snapshot metadata
│   ├── ingestion_assumptions.md
│   ├── manifest_schema.json   # Schema for run manifests
│   └── DEVELOPER.md
├── environment.yml             # Conda env (advds); single source of truth for dependencies
├── README.md
├── CONTRIBUTING.md
├── EDA_Guidelines.md           # Guardrails for EDA (what you may/may not change)
└── LICENSE
```

### Design Principles

1. **Separation of concerns**: Ingestion, validation, and cleaning are separate scripts; validation does not mutate data.
2. **Project-root-relative paths**: Scripts resolve paths from the repo root (via `Path(__file__).parent` / `get_project_root()`), so they work from any working directory.
3. **Reproducibility**: Environment is defined in `environment.yml`; only canonical data files are tracked; everything else is regenerable from scripts.
4. **Conda-only**: This project uses Conda (env `advds`). Virtual environments (venv) are not supported.

---

## Module Documentation

### Scripts (in `scripts/`)

| Script | Purpose | Key outputs |
|--------|---------|-------------|
| `data_ingestion.py` | Fetch GDELT articles and OHLCV prices; archive existing canonical raw files; write run manifest | `data/raw/gdelt_articles.csv`, `data/raw/prices_daily.csv`, `data/raw/snapshots/run_manifest_YYYY-MM-DD.json`, `data/raw/archive/*.csv` |
| `validate_gdelt.py` | Validate GDELT schema, coverage, missingness; report only | `docs/validation/gdelt_articles_validation.md` |
| `cleaning_gdelt.py` | Dedupe by URL + headline, filter language/relevance, drop cols; overwrites output | `data/processed/gdelt_articles_clean.csv` |
| `ohlcv_validation.py` | Validate OHLCV schema, trading days, outliers; report only | `docs/validation/ohlcv_validation.md` |
| `ohlcv_cleaning.py` | Normalize types, apply market calendar, fix logical prices, interpolate; overwrites output | `data/processed/prices_daily_clean.csv` |
| `accumulate.py` | Merge new cleaned snapshot + existing accumulated; dedupe by key; sort by date; write manifest | `data/processed/gdelt_articles_accumulated.csv`, `prices_daily_accumulated.csv`, `*_manifest.json` |
| `run_pipeline.sh` | Run validation → cleaning → accumulation; optionally run ingestion when `RUN_INGEST=1` | Depends on steps run |

### Important conventions

- **Paths**: Use `get_project_root()` (or equivalent) and build paths from the project root. Do not rely on the current working directory.
- **Canonical raw files**: Before overwriting `data/raw/gdelt_articles.csv` or `data/raw/prices_daily.csv`, `data_ingestion.py` moves the existing file to `data/raw/archive/{dataset}_{YYYY-MM-DD}.csv`.
- **Processed files**: Cleaning scripts overwrite `data/processed/*.csv`; there is no archive step for processed data.
- **Run manifest**: Written only when `data_ingestion.py` runs (e.g. via `RUN_INGEST=1`). Schema: `docs/manifest_schema.json`.

---

## Data Pipeline

### Flow

1. **Ingestion** (optional per run): `data_ingestion.py`  
   Archives existing canonical raw files → fetches new data → writes `gdelt_articles.csv`, `prices_daily.csv` → writes `data/raw/snapshots/run_manifest_YYYY-MM-DD.json`.

2. **Validation** (no mutation): `validate_gdelt.py`, `ohlcv_validation.py`  
   Read raw (or processed) inputs; write reports under `docs/validation/`. Reports are gitignored.

3. **Cleaning** (overwrites): `cleaning_gdelt.py`, `ohlcv_cleaning.py`  
   Read raw inputs → apply deterministic cleaning → overwrite `data/processed/*.csv`.

### Tracked vs local artifacts

- **Tracked in git**: `data/raw/gdelt_articles.csv`, `data/raw/prices_daily.csv`, `data/processed/gdelt_articles_clean.csv` (and optionally other canonical processed files per team policy).
- **Gitignored (local only)**: `data/raw/archive/*`, `data/raw/snapshots/*`, `docs/validation/*`, run manifests.

See `docs/data_snapshot_log.md` and the README for the full data policy.

---

## Development Workflow

### 1. Clone and enter repo

```bash
git clone <repository-url>
cd market-sentiment-analysis
```

### 2. Create and activate Conda environment

```bash
conda env create -f environment.yml
conda activate advds
```

Python 3.11 and all dependencies (including `pandas`, `numpy`, `pandas_market_calendars`, `yfinance`, `requests`) are installed from `environment.yml`. Do not use `requirements.txt`; it is not used for this project.

### 3. (Linux/macOS) Make pipeline script executable

```bash
chmod +x scripts/run_pipeline.sh
```

### 4. Verify setup

```bash
python -c "import pandas, numpy, pandas_market_calendars, yfinance, requests; print('Environment OK')"
```

Or run the pipeline; it will check for `pandas`, `numpy`, and `pandas_market_calendars` and exit with a clear message if any are missing.

### 5. Run the pipeline

- **Validation + cleaning only** (raw files must already exist):

  ```bash
  ./scripts/run_pipeline.sh
  # or: bash scripts/run_pipeline.sh
  ```

- **Including ingestion** (fetch new raw data, archive previous, write run manifest):

  ```bash
  RUN_INGEST=1 ./scripts/run_pipeline.sh
  ```

### 6. Run individual scripts (optional)

All from repo root (or any directory; paths are project-root-relative):

```bash
python scripts/data_ingestion.py
python scripts/validate_gdelt.py
python scripts/cleaning_gdelt.py
python scripts/ohlcv_validation.py
python scripts/ohlcv_cleaning.py
```

---

## Code Style & Standards

- **Style**: Follow PEP 8. Prefer type hints and docstrings (e.g. Google-style) for public functions.
- **Paths**: Use `pathlib.Path` and resolve from project root (e.g. `get_project_root()`); avoid hardcoded absolute paths.
- **Naming**: `snake_case` for functions/variables; `UPPER_SNAKE_CASE` for constants; `snake_case.py` for script names.
- **Data**: Validation scripts must not modify raw or processed files; cleaning scripts read from raw (or validated inputs) and write only to `data/processed/`.

See [CONTRIBUTING.md](../CONTRIBUTING.md) for workflow (tickets, branching, commits).

---

## Common Tasks

### Run full pipeline with ingestion

```bash
conda activate advds
RUN_INGEST=1 ./scripts/run_pipeline.sh
```

### Run only validation and cleaning (no ingestion)

```bash
conda activate advds
./scripts/run_pipeline.sh
```

### Read cleaned data (e.g. in a notebook or script)

```python
from pathlib import Path
import pandas as pd

root = Path(__file__).resolve().parents[1]  # or your project-root logic
gdelt = pd.read_csv(root / "data/processed/gdelt_articles_clean.csv", parse_dates=["seendate"])
prices = pd.read_csv(root / "data/processed/prices_daily_clean.csv", parse_dates=["date"])
```

Use **cleaned** data only for EDA/analysis. See [EDA_Guidelines.md](../EDA_Guidelines.md) for what you may and may not do in EDA.

### Add a new script

- Add it under `scripts/` (e.g. `scripts/my_script.py`).
- Resolve paths from project root (reuse or mirror `get_project_root()` from `data_ingestion.py` or `validate_gdelt.py`).
- If it writes outputs, use `data/processed/` for derived data; do not write into `data/raw/` except via the canonical ingestion flow.
- Document the script and any new outputs in this file and in the README if user-facing.

### Update the data snapshot log

After updating canonical data, edit `docs/data_snapshot_log.md` manually (date range, row counts, notes). There is no automated update for the snapshot log.

---

## Troubleshooting

### "No conda environment active"

The pipeline requires an active Conda environment (it checks `CONDA_PREFIX`).

**Fix:** Activate the project env before running the pipeline:

```bash
conda activate advds
./scripts/run_pipeline.sh
```

### "Permission denied" when running `./scripts/run_pipeline.sh`

**Fix:** Make the script executable once:

```bash
chmod +x scripts/run_pipeline.sh
```

Or run it via bash: `bash scripts/run_pipeline.sh`.

### "Run manifest was not created"

The run manifest is written only when **ingestion** runs (e.g. `RUN_INGEST=1`). If you run the pipeline without `RUN_INGEST=1`, no manifest is produced.

**Fix:** To generate a manifest, run:

```bash
RUN_INGEST=1 ./scripts/run_pipeline.sh
```

### "data/raw/gdelt_articles.csv was not found" (or prices_daily.csv)

The pipeline expects existing raw files when ingestion is **not** run.

**Fix:** Either run with ingestion once to create them, or ensure the repo has the canonical raw files (e.g. from a prior run or from another branch that tracks them).

### Missing Python packages (e.g. pandas_market_calendars)

**Fix:** Recreate the environment from the project’s single source of truth:

```bash
conda env create -f environment.yml --force
conda activate advds
```

Or install the missing package in the active env:

```bash
conda activate advds
conda install -c conda-forge pandas_market_calendars
# or: pip install pandas_market_calendars
```

### Script writes to wrong directory (e.g. `scripts/data/` or `scripts/docs/`)

Scripts must resolve paths from the **project root**, not the current working directory.

**Fix:** Use a `get_project_root()` helper (see `data_ingestion.py` or `validate_gdelt.py`) and build all paths from that root (e.g. `root / "data" / "raw"`).

---

## API Reference

### GDELT Doc 2.0 API

News article search and metadata used by `data_ingestion.py` for GDELT article fetches.

- **Official documentation:** [GDELT Doc 2.0 API Debuts](https://blog.gdeltproject.org/gdelt-doc-2-0-api-debuts/) (GDELT Project Blog)

**Relevant details for this project:**

- **Base URL:** `https://api.gdeltproject.org/api/v2/doc/doc`
- **Date range:** The API searches a **rolling window of the last 3 months** of coverage. You can use `STARTDATETIME` / `ENDDATETIME` (format `YYYYMMDDHHMMSS`) to specify a window within the last 3 months, or `TIMESPAN` (e.g. `1week`, `5d`, `12h`) for an offset from the present.
- **Modes:** We use `mode=artlist` for article lists. Other modes include timelines, tone charts, and image collages.
- **Sort:** `sort=datedesc` (newest first) or `dateasc` (oldest first). Our ingestion uses `datedesc` and caps at `max_articles_per_company`, so returned articles are skewed to the recent end of the requested window—see [docs/gdelt_vs_prices_date_range_origin.md](gdelt_vs_prices_date_range_origin.md).
- **Limits:** `MAXRECORDS` (default 75, max 250 per request) controls how many results are returned per request; we paginate via `startrecord`.

Query syntax (exact phrases, OR, domain, tone, etc.) and full parameter list are in the official post above.

---

## Additional Resources

- [README.md](../README.md) — Overview, getting started, pipeline usage
- [CONTRIBUTING.md](../CONTRIBUTING.md) — Workflow, tickets, branching, data handling
- [EDA_Guidelines.md](../EDA_Guidelines.md) — What EDA may do and what is off-limits
- [docs/data_snapshot_log.md](data_snapshot_log.md) — What is tracked and snapshot metadata
- [docs/manifest_schema.json](manifest_schema.json) — Schema for run manifests
