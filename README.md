# Market Sentiment Analysis

## Overview
This capstone project explores the relationship between public sentiment and
short-term market reactions for leading Nasdaq technology companies (often
referred to as the “Magnificent Seven”).

The project is **exploratory in nature** and focuses on identifying patterns,
correlations, and limitations in sentiment-based analysis rather than attempting
to predict future stock prices.

This repository is structured to support:
- deterministic data ingestion
- explicit data validation and cleaning steps
- repeatable exploratory analysis
- team-based development with clear process boundaries

## Table of Contents
- [Project Goals](#project-goals)
- [Scope & Non-Goals](#scope--non-goals)
- [Repository Structure](#repository-structure)
- [Data Lifecycle](#data-lifecycle)
- [Getting Started](#getting-started)
- [Workflow](#workflow)
- [Contributing](#contributing)
- [Notes](#notes)

---

## Project Goals
- Build reproducible data ingestion pipelines
- Collect and preprocess public sentiment data from news and/or social sources
- Align sentiment signals with short-term market behavior
- Analyze patterns and limitations across companies and time windows
- Practice Agile workflows in a team-based data science project

---

## Scope & Non-Goals
**In scope**
- Exploratory sentiment analysis
- Publicly available data sources
- Daily-level aggregation and comparison
- Transparent assumptions and limitations

**Out of scope**
- Price prediction or trading strategies
- Real-time systems or production deployment
- Claims of causality or financial advice

---

## Repository Structure
```text
market-sentiment-analysis/
├── data/
│   ├── raw/                    # Canonical raw data (tracked)
│   │   ├── gdelt_articles.csv
│   │   ├── prices_daily.csv
│   │   ├── archive/            # Date-stamped backups (gitignored)
│   │   └── snapshots/          # Run manifests run_manifest_YYYY-MM-DD.json (gitignored)
│   └── processed/             # Cleaned + accumulated outputs
│       ├── gdelt_articles_clean.csv
│       ├── gdelt_articles_accumulated.csv
│       ├── gdelt_articles_with_sentiment.csv   # After add_sentiment / dedupe_and_sentiment
│       ├── prices_daily_clean.csv
│       ├── prices_daily_accumulated.csv
│       └── gdelt_ohlcv_join.csv               # After build_gdelt_ohlcv_join.py (for price–news analysis)
├── scripts/
│   ├── data_ingestion.py      # Data ingestion (GDELT articles + OHLCV prices)
│   ├── validate_gdelt.py       # GDELT data validation
│   ├── cleaning_gdelt.py      # GDELT data cleaning (URL + headline dedupe)
│   ├── ohlcv_validation.py    # OHLCV price data validation
│   ├── ohlcv_cleaning.py      # OHLCV price data cleaning
│   ├── accumulate.py         # Merge new cleaned + existing accumulated; dedupe; sort by date
│   ├── add_sentiment.py      # Add sentiment scores to GDELT (word-bank lexicon)
│   ├── dedupe_and_sentiment.py  # Dedupe accumulated GDELT + regenerate sentiment → gdelt_articles_with_sentiment.csv
│   ├── build_gdelt_ohlcv_join.py # Join GDELT (with sentiment) to OHLCV (news day t → prices day t+1)
│   └── run_pipeline.sh        # Full pipeline: validate → clean → accumulate; optional ingestion (RUN_INGEST=1)
├── notebooks/                 # Exploratory notebooks (root)
├── docs/
│   ├── architecture/          # Pipeline diagram (pipeline.md, pipeline.svg)
│   ├── eda/                   # EDA notebooks and summaries by sprint (e.g. sprint_2/notebooks/)
│   ├── findings/              # Written findings (e.g. weekend_effect_analysis.md)
│   ├── validation/            # Validation reports (gitignored; regenerated by scripts)
│   ├── data_snapshot_log.md   # Log of canonical snapshot metadata
│   ├── ingestion_assumptions.md
│   └── manifest_schema.json  # Schema for run manifests
├── environment.yml            # Conda environment (advds) — single source of truth for dependencies
├── README.md
├── CONTRIBUTING.md
├── EDA_Guidelines.md          # Guardrails for EDA
└── LICENSE
```
## Data Lifecycle
1. Raw Data (Immutable)
- Stored under `data/raw`
- Treated as **append-only**
- Never manually edited
- May be re-fetched, but prior snapshots are preserved
2. Validation (Non-Mutating)
- Validation scripts:
    - **Inspect** raw data
    - **Report** gaps, duplicates, anomalies
    - **Do not** modify raw files
- Outputs:
    - console summaries
    - optional markdown/csv reports under `docs/validation/` (local artifacts, gitignored; regenerate by running validation scripts)
3. Cleaning (Deterministic Mutation)
- Cleaning scripts:
    - operate only on **validated raw data**
    - apply deterministic transformations:
        - deduplication
        - language filtering
        - relevance filtering
        - column pruning
        - missing value handling
    - write outputs to `data/processed/`
- *Cleaned data may be overwritten, as it can always be regenerated from raw inputs*
- *Cleaning logic is implemented in standalone scripts under `scripts/clean_*.py` and does not perform ingestion or validation.*
4. Analysis (Exploratory Only)
  - Conducted in notebooks under `notebooks/`
  - Uses cleaned datasets only
  - Focuses on visualization and pattern exploration
  - No modeling or inference claims at this stage
 
### Accumulation Strategy
- Each ingestion run pulls data "up to today"
- Raw outputs are date-stamped
- Cleaned datasets are regenerated as needed
- *Incremental append and automation will be revisited **after validation and cleaning logic is finalized***
---
### Raw Snapshot Naming Convention

To avoid accidental data accumulation and maintain clarity:

**Canonical files** (tracked in git):
- `data/raw/gdelt_articles.csv`
- `data/raw/prices_daily.csv`

**Archived snapshots** (local only, not committed):
- Location: `data/raw/archive/`
- Format: `{dataset}_{YYYY-MM-DD}.csv`
- Examples:
  - `data/raw/archive/gdelt_articles_2026-01-26.csv`
  - `data/raw/archive/prices_daily_2026-01-26.csv`

**Rules:**
1. Only canonical files are committed to `main`
2. Before overwriting a canonical file, move the current version to `archive/` with a date suffix
3. Archive folder is gitignored to prevent accidental commits
4. See `docs/data_snapshot_log.md` for snapshot metadata tracking

### Run Manifests

Each data ingestion run generates a manifest file to track metadata for traceability and debugging.

**Location:** `data/raw/snapshots/run_manifest_YYYY-MM-DD.json`

**Schema:** See [`docs/manifest_schema.json`](docs/manifest_schema.json)

**Fields:**
| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `timestamp` | string | Yes | ISO 8601 datetime of the run |
| `tickers_covered` | array | Yes | List of ticker symbols included |
| `row_counts` | object | Yes | Row counts keyed by dataset name |
| `script_version` | string | No | Version of ingestion script |
| `git_commit` | string | Yes | Short git commit hash |
| `notes` | string | No | Optional free-text notes |

**Example:**
```json
{
  "timestamp": "2026-01-26T14:32:00Z",
  "tickers_covered": ["AAPL", "MSFT", "NVDA", "GOOGL", "AMZN", "META", "TSLA"],
  "row_counts": {
    "gdelt_articles": 1400,
    "prices_daily": 133
  },
  "script_version": "1.0.0",
  "git_commit": "cc6c3b4",
  "notes": ""
}
```

**Notes:**
- Manifest files are gitignored (local only)
- Generated during manual or automated ingestion runs
- Used for debugging and audit trails
---
## Getting Started

### Environment Setup

**This project requires Anaconda/Conda** - Python 3.11 is recommended.

**Note:** This project is configured for conda environments. Using virtual environments (venv) may cause conflicts with package management and path resolution.

The repository includes an `environment.yml` file that defines the full environment (conda and pip dependencies) for consistent setup across the team.

```bash
# Create and activate the environment from environment.yml
conda env create -f environment.yml
conda activate advds
```

This creates the `advds` environment with Python 3.11 and all required packages. The `environment.yml` is the single source of truth for dependencies; `requirements.txt` is not used.

### Running Scripts

All scripts use project-root-relative paths and can be run from any directory.

**Note for Linux/macOS users:** The pipeline script (`run_pipeline.sh`) needs to be executable. If you encounter a "Permission denied" error, run:
```bash
chmod +x scripts/run_pipeline.sh
```

#### Individual Scripts
```bash
# Ingest data (GDELT articles + OHLCV prices)
python scripts/data_ingestion.py

# Validate GDELT data
python scripts/validate_gdelt.py

# Clean GDELT data
python scripts/cleaning_gdelt.py

# Validate OHLCV price data
python scripts/ohlcv_validation.py

# Clean OHLCV price data
python scripts/ohlcv_cleaning.py
```

#### Full Pipeline (Recommended)
Run the complete validation and cleaning pipeline:

**Linux/macOS:**
```bash
# Make the script executable (first time only)
chmod +x scripts/run_pipeline.sh

# Ensure conda environment is activated
conda activate advds

# Run the pipeline
./scripts/run_pipeline.sh

# Or run with bash explicitly
bash scripts/run_pipeline.sh

# To include data ingestion in the pipeline:
RUN_INGEST=1 ./scripts/run_pipeline.sh
```

**Windows:**
```bash
# Ensure conda environment is activated
conda activate advds

# Using Git Bash or WSL
bash scripts/run_pipeline.sh

# Using WSL (Windows Subsystem for Linux)
# First, make executable:
chmod +x scripts/run_pipeline.sh
# Then run:
./scripts/run_pipeline.sh

# To include data ingestion:
RUN_INGEST=1 bash scripts/run_pipeline.sh
```

**Important:** The pipeline script requires an active conda environment. It will check for `CONDA_PREFIX` and exit with an error if no conda environment is active. The script also validates that required Python packages are installed.

The pipeline script:
- Validates dependencies are installed
- **When `RUN_INGEST=1`:** Runs `data_ingestion.py` (fetches raw data, archives previous canonical files, writes a run manifest to `data/raw/snapshots/run_manifest_YYYY-MM-DD.json`)
- When ingestion is skipped (`RUN_INGEST=0`, the default), no run manifest is produced; raw files must already exist
- Runs validation scripts (generates reports in `docs/validation/`)
- Runs cleaning scripts (generates cleaned data in `data/processed/`)
- Verifies all outputs are created successfully

#### Sequence for sentiment and price–news join

To produce **`gdelt_articles_with_sentiment.csv`** and then the **GDELT–OHLCV join table** (`gdelt_ohlcv_join.csv`) for analysis, run steps in this order:

1. **Pipeline** (validation, cleaning, accumulation):
   ```bash
   ./scripts/run_pipeline.sh
   # Optional: RUN_INGEST=1 ./scripts/run_pipeline.sh  # to fetch new raw data first
   ```
   Produces: `gdelt_articles_clean.csv`, `gdelt_articles_accumulated.csv`, `prices_daily_clean.csv`, `prices_daily_accumulated.csv`.

2. **Add sentiment** to the accumulated GDELT articles:
   ```bash
   python scripts/add_sentiment.py
   ```
   Reads `gdelt_articles_accumulated.csv`, adds `sentiment_score` (and related columns), writes `gdelt_articles_with_sentiment.csv`. Default paths: `--input` / `--output` in `data/processed/`.

3. **Deduplicate and regenerate sentiment** (recommended before the join):
   ```bash
   python scripts/dedupe_and_sentiment.py
   ```
   Reads `gdelt_articles_accumulated.csv`, deduplicates by URL and by normalized headline per ticker, then adds sentiment and overwrites `gdelt_articles_with_sentiment.csv`. Run this so the join uses deduplicated articles.

4. **Build the GDELT–OHLCV join table** (news day t → prices day t+1):
   ```bash
   python scripts/build_gdelt_ohlcv_join.py
   ```
   Reads `gdelt_articles_with_sentiment.csv` and `prices_daily_accumulated.csv`, aligns each article to the *next trading day* (weekends/holidays handled via NYSE calendar), and writes `data/processed/gdelt_ohlcv_join.csv` for downstream analysis.

**Summary:** Pipeline → `add_sentiment.py` → `dedupe_and_sentiment.py` → `build_gdelt_ohlcv_join.py`. The join script must run after `gdelt_articles_with_sentiment.csv` exists (after step 2 or 3); running step 3 before the join is recommended so the join uses deduplicated sentiment data.

## Validation Scripts

Each dataset has a dedicated validation script following a shared structure:
- **`validate_gdelt.py`** - Validates GDELT article data
- **`ohlcv_validation.py`** - Validates OHLCV price data

Validation scripts:
- Audit data quality and coverage
- **Do not mutate** datasets
- Generate markdown reports in `docs/validation/`
- Can be run from any directory (paths are project-root-relative)

## Workflow
This project follows an **Agile sprint-based workflow**:
- Work is tracked using GitHub Issues and Projects
- Each sprint includes planning, daily scrums, review and retrospective
- Tickets define **scope**, **ownership**, and **definitions of done**

## Contributing
Collaboration guidelines and workflow expectations are documented in
[CONTRIBUTING.md](CONTRIBUTING.md).

## Notes
- **This project requires Anaconda/Conda** - virtual environments (venv) are not supported due to package management conflicts
- Generated data artifacts are intentionally excluded from version control
- All results should be reproducible from committed code
- Assumptions and limitations are documented throughout the project
- Scripts use project-root-relative paths and work correctly regardless of current working directory
- The project uses `pandas_market_calendars` for market calendar validation

---

